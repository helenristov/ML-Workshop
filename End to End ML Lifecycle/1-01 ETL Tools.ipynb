{"cells":[{"cell_type":"markdown","source":["![x](https://zdnet4.cbsistatic.com/hub/i/r/2017/12/17/e9b8f576-8c65-4308-93fa-55ee47cdd7ef/resize/370xauto/30f614c5879a8589a22e57b3108195f3/databricks-logo.png)"],"metadata":{}},{"cell_type":"markdown","source":["&copy; 2019 Databricks, Inc. All rights reserved.<br/>"],"metadata":{}},{"cell_type":"markdown","source":["### ETL Tools\n\nIn computing, ETL stands for Extract-Transform-Load and it refers to the process used to collect data from numerous disparate databases, applications and systems, transforming the data so that it matches the target systemâ€™s required formatting and loading it into a destination database. (https://databricks.com/glossary/extract-transform-load)\n\nIn today's world, as cloud adoption increases, organizations tend to work with Enterprise Data Integration Tools in order to ensure that their data is readily available on their systems. \n\nThis Medium article does a great job going into a lot of details of what the ETL process truly contains: https://medium.com/hashmapinc/etl-understanding-it-and-effectively-using-it-f827a5b3e54d"],"metadata":{}},{"cell_type":"markdown","source":["#### Gartner Magic Quadrant\n\nBased on Gartner's last [MQs](https://en.wikipedia.org/wiki/Magic_Quadrant) (a series of market research reports published yearly), you tend to have a variety of tools in this market, specifically dedicated towards Enterprise ETL, whether it's purely dedicateed to data integration, or to Integration Platform as a Service.\n\n![s1](https://www.informatica.com/content/dam/informatica-com/global/amer/us/image/misc/data-integration-magic-quadrant-2018.jpg)\n\n![s](https://www.informatica.com/content/dam/informatica-com/global/amer/us/image/misc/ipaas-magic-quadrant-2019.jpg)"],"metadata":{}},{"cell_type":"markdown","source":["Different parts of the ETL process can be handled by the same tool, or by a multitude of tools. While in Azure there is a focus on building ETL pipelines through a few specific services (usually storage solutions + Azure Data Factory + Azure Databricks), there are a whole variety of companies which focus on particular segments of this market."],"metadata":{}},{"cell_type":"markdown","source":["#### Data Ingestion\n\nCompanies such as Attunity, FiveTran, Segment.io, Streamsets, Striim focus exclusively on the ingestion side, and how companies can easily load data from their on-going data sources, or how they can simply and efficiently re-house their current data into big data platforms."],"metadata":{}},{"cell_type":"markdown","source":["#### Data Transformation\n\nIn this space, there are a lot of companies providing a visual user interface for data preparation activities. Tools such as Alteryx, Talend, Informatica, Datameer, SnapLogic, Trifacta, Tamr, Paxata, all help their users perform visual preparation activities in order to have data clean and ready to be used by business owners."],"metadata":{}},{"cell_type":"markdown","source":["#### Governance and Security\n\nLast but not least, when building ETL pipelines, governance and security are key to ensure that the process is transparent, complies to regulatory requirements, and data can always be traced between its source and its final usage. Companies operating in this space, such as Informatica, Alation, Unifi Software, Collibra, CompactBI, Waterline, all ensure that data catalogued and meta-information is readily available for end-users."],"metadata":{}},{"cell_type":"markdown","source":["BlueGranite, a consulting services provider, has very clearly taken the steps of Extract, Transform, and Load, and mapped it out to utilisation of Azure Databricks.\n\nParaphrasing their recent blog post, Azure Databricks can tackle a big part of the ETL ecosystem:\n\nExtract\n* out of the box connections to Azure storage systems\n* easy additional connectivity through JDBC\n* VNET setup to address on-premise data sources\n* direct connections to streaming data sources such as Event Hubs or Kafka\n\nTransform\n* simple to use DataFrames API\n* variety of coding languages, interchangable on the fly\n* both batch and real-time capabilities\n* massive performance optimisations\n\nLoad\n* built-in environment of tables on top of data lakes\n* JDBC/ODBC connectivity to BI Tools such as PowerBI\n* easy and performant file storage and compute via Delta Lakes\n\nAll the above is happening within a fully integrated and secure environment guaranteed by Azure Active Directory, and any additional credentials can be safely managed using Azure Key Vault.\n\nhttps://www.blue-granite.com/blog/azure-databricks-spark-etl-unifying-data-engineering-at-cloud-scale"],"metadata":{}},{"cell_type":"markdown","source":["Finally, for a video recap of doing Data Engineering with Azure Databricks, you can review a video from Microsoft Connect where Richard Garris talks about common usage scenarios of ADB and provides a quick overview of the platform.\n\n[![ADB](https://img.youtube.com/vi/lquF9x8Lw8E/0.jpg)](https://www.youtube.com/watch?v=lquF9x8Lw8E)\n\nAlso, for streaming ETL processes, this is also a good read: https://medium.com/microsoftazure/an-introduction-to-streaming-etl-on-azure-databricks-using-structured-streaming-databricks-16b369d77e34"],"metadata":{}},{"cell_type":"markdown","source":["# ADF integration"],"metadata":{}},{"cell_type":"markdown","source":["## Next Step\n\n[Extracting Data]($./1-02 Extracting Data)"],"metadata":{}},{"cell_type":"markdown","source":["&copy; 2019 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"1-01 ETL Tools","notebookId":1685127807621922},"nbformat":4,"nbformat_minor":0}
