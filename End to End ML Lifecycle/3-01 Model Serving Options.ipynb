{"cells":[{"cell_type":"markdown","source":["![x](https://zdnet4.cbsistatic.com/hub/i/r/2017/12/17/e9b8f576-8c65-4308-93fa-55ee47cdd7ef/resize/370xauto/30f614c5879a8589a22e57b3108195f3/databricks-logo.png)"],"metadata":{}},{"cell_type":"markdown","source":["&copy; 2019 Databricks, Inc. All rights reserved.<br/>"],"metadata":{}},{"cell_type":"markdown","source":["### Model Serving Options\n\nWhen we move to talking about actually operationalizing the machine learning models we've build so far is where the discussion becomes tricky. Many organizations have not yet reached this step, as it can become quite complex to get here, and this part is not any easier either."],"metadata":{}},{"cell_type":"markdown","source":["When we talk about open source deployment of ML models, the first to stand out is Docker. Docker is a tool designed to make it easier to create, deploy, and run applications by using containers. Containers allow a developer to package up an application with all of the parts it needs, such as libraries and other dependencies, and ship it all out as one package. By doing so, thanks to the container, the developer can rest assured that the application will run on any other Linux machine regardless of any customized settings that machine might have that could differ from the machine used for writing and testing the code.\n\nhttps://opensource.com/resources/what-docker"],"metadata":{}},{"cell_type":"markdown","source":["As the extent of the scope of Docker containers grew, so did the complexity of their deployment. As such, Kubernetes quickly came into the light of many organizations. Kubernetes is a portable, extensible open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\n\nhttps://kubernetes.io/docs/concepts/overview/what-is-kubernetes/"],"metadata":{}},{"cell_type":"markdown","source":["Using these 2 solutions, some organizations do realise their deployments, however, with a lengthy development cycle, and a rigourous maintainence team behind it. However, to save some of the headaches, make the process smoother for more users (rather than just expert engineering teams), and in some cases save on TCO too, you also have alternatives in Azure."],"metadata":{}},{"cell_type":"markdown","source":["By using Databricks to create your models (or, alternatively, some of the other solutions mentioned in the previous section of our workshop), you can then choose your serving layer. Whether that's batch (where you scroll data on a regular interval), streaming (scoring data non-stop), or via REST API (where you make \"random\" calls to be scored), you can achieve the first 2 options using Databricks directly (or, for more complex pipelines, using scheduling via Azure Data Factory), while the latter can easily be covered by integrating Databricks with Azure's Machine Learning Service, for an easy way to deploy to an auto-scalable, containerized API."],"metadata":{}},{"cell_type":"markdown","source":["Again, another community reference post on the above: https://towardsdatascience.com/how-to-bring-your-data-science-project-in-production-b36ae4c02b46"],"metadata":{}},{"cell_type":"markdown","source":["## Next Step\n\n[Serving in Batch & Streaming]($./3-02 Serving in Batch & Streaming)"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2019 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"3-01 Model Serving Options","notebookId":1685127807621607},"nbformat":4,"nbformat_minor":0}
