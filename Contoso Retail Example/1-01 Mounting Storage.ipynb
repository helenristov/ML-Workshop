{"cells":[{"cell_type":"markdown","source":["<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://cdn2.hubspot.net/hubfs/438089/docs/training/dblearning-banner.png\" alt=\"Databricks Learning\" width=\"555\" height=\"64\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["&copy; 2018 Databricks, Inc. All rights reserved.<br/>"],"metadata":{}},{"cell_type":"markdown","source":["#![Spark Logo Tiny](https://kpistoropen.blob.core.windows.net/collateral/roadshow/logo_spark_tiny.png) Getting Started with Azure Storage and Azure Data Lake \n\n**Databricks Mount Points:**\n- Connect to our Azure Storage Account - https://docs.azuredatabricks.net/spark/latest/data-sources/azure/azure-storage.html\n- Connect to our Azure Data Lake - https://docs.azuredatabricks.net/spark/latest/data-sources/azure/azure-datalake.html"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://kpistoropen.blob.core.windows.net/collateral/roadshow/logo_spark_tiny.png) Connect to our Azure Storage Account\n\nNext, let's connect to the read-only Blob store you'll have access to for data needed in this course.  We can easily mount data in blob stores to Azure Databricks for fast and scalable data storage\n\n*Note:* You will have to have a cluster running to execute this code"],"metadata":{}},{"cell_type":"markdown","source":["**IMPORTANT** This notebook must be run using Azure Databricks runtime 4.0 or better."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.help()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\nthis package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\nanother FileSystem URI.\n\nFor more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n\nIn notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\nstraightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\ntranslates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n    <h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /><h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><br /></div>"]}}],"execution_count":6},{"cell_type":"code","source":["#In case you have run this training before, you can unmount in order to be able to re-mount\ntry:\n  dbutils.fs.unmount(\"/mnt/databricks-workshop-datasets\") # Use this to unmount as needed\nexcept:\n  print(\"{} already unmounted\".format(\"/mnt/databricks-workshop-datasets\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/databricks-workshop-datasets/ has been unmounted.\n<span class=\"ansired\">Out[</span><span class=\"ansired\">4</span><span class=\"ansired\">]: </span>True\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# This only needs to be ran once, globably. Once we have mounted the storage account no need need to do it again (unless you unmount). \n# These credentials DO NOT have write access\n\nSTORAGE_ACCOUNT = \"channelsapublicprodblob\"\nCONTAINER = \"channelsa-datasets\"\nMOUNT_POINT = \"/mnt/databricks-workshop-datasets\"\nSAS_KEY = \"?sv=2018-03-28&ss=b&srt=sco&sp=rwlac&se=2022-07-07T02:01:53Z&st=2019-03-24T19:01:53Z&spr=https&sig=o6rvr92oZH4nzdn7r4gR%2Bxv%2Fj%2BkOgv5BhXIfbTJYM%2Bg%3D\"\n\n#Define strings to be passed to the mount function\nsource_str = \"wasbs://{container}@{storage_acct}.blob.core.windows.net/\".format(container=CONTAINER, storage_acct=STORAGE_ACCOUNT)\nconf_key = \"fs.azure.sas.{container}.{storage_acct}.blob.core.windows.net\".format(container=CONTAINER, storage_acct=STORAGE_ACCOUNT)\n\n#Run the mount function using template in the documentation\ntry:\n  dbutils.fs.mount(\n    source = source_str,\n    mount_point = MOUNT_POINT,\n    extra_configs = {conf_key: SAS_KEY}\n  )\nexcept Exception as e:\n  print(\"ERROR: {} already mounted. Run previous cells to unmount first\".format(MOUNT_POINT))\n  \n#If needed to unmount, use this:\n#try:\n#  dbutils.fs.unmount(MOUNT_POINT) # Use this to unmount as needed\n#except:\n#  print(\"{} already unmounted\".format(MOUNT_POINT))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["%fs ls /mnt/databricks-workshop-datasets/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/databricks-workshop-datasets/Contoso-retail/</td><td>Contoso-retail/</td><td>0</td></tr><tr><td>dbfs:/mnt/databricks-workshop-datasets/Example1folder/</td><td>Example1folder/</td><td>0</td></tr><tr><td>dbfs:/mnt/databricks-workshop-datasets/Example2folder/</td><td>Example2folder/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":9},{"cell_type":"code","source":["%fs ls /mnt/databricks-workshop-datasets/Contoso-retail/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/databricks-workshop-datasets/Contoso-retail/dev/</td><td>dev/</td><td>0</td></tr><tr><td>dbfs:/mnt/databricks-workshop-datasets/Contoso-retail/initech/</td><td>initech/</td><td>0</td></tr><tr><td>dbfs:/mnt/databricks-workshop-datasets/Contoso-retail/structured-streaming/</td><td>structured-streaming/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["If, by any chance, you cannot write to the local FileStore, you can use this mounted blob below"],"metadata":{}},{"cell_type":"code","source":["STORAGE_ACCOUNT = \"channelsapublicexercises\"\nCONTAINER = \"exercise-container\"\nMOUNT_POINT = \"/mnt/databricks-workshop-exercises\"\nSAS_KEY = \"?sv=2018-03-28&ss=b&srt=sco&sp=rwdlac&se=2021-11-27T03:35:07Z&st=2019-03-24T19:35:07Z&spr=https&sig=w%2Fp9iG2FGDlgNT716Kt3ZFnWQuUGlaxz3Bu4yVAVEwo%3D\"\n\n#Define strings to be passed to the mount function\nsource_str = \"wasbs://{container}@{storage_acct}.blob.core.windows.net/\".format(container=CONTAINER, storage_acct=STORAGE_ACCOUNT)\nconf_key = \"fs.azure.sas.{container}.{storage_acct}.blob.core.windows.net\".format(container=CONTAINER, storage_acct=STORAGE_ACCOUNT)\n\n#Run the mount function using template in the documentation\ntry:\n  dbutils.fs.mount(\n    source = source_str,\n    mount_point = MOUNT_POINT,\n    extra_configs = {conf_key: SAS_KEY}\n  )\nexcept Exception as e:\n  print(\"ERROR: {} already mounted. Run previous cells to unmount first\".format(MOUNT_POINT))\n  \n#If needed to unmount, use this:\n#try:\n#  dbutils.fs.unmount(MOUNT_POINT) # Use this to unmount as needed\n#except:\n#  print(\"{} already unmounted\".format(MOUNT_POINT))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["## Next Step\n\n[Reading Data]($../2-ETL/2-01 Reading Data)"],"metadata":{}},{"cell_type":"markdown","source":["&copy; 2018 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"1-01 Mounting Storage","notebookId":1685127807621331},"nbformat":4,"nbformat_minor":0}
